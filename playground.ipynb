{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is building the sentence autoencoder using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "variational autoencoder\n",
    "Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "\n",
    "batch_size = 16\n",
    "original_dim = 784  # sentence length\n",
    "latent_dim = 2\n",
    "intermediate_dim = 128  # encode size\n",
    "epsilon_std = 0.01\n",
    "nb_epoch = 40\n",
    "\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_std = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_std = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_std) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_std])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_std])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_std - K.square(z_mean) - K.exp(z_log_std), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_mean) \n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae.fit(_, _,\n",
    "        shuffle=True,\n",
    "        nb_epoch=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(_, _),\n",
    "        callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is building the sentence siamese network using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras.layers import initializations\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# multi-sense vectors\n",
    "class MultiSense(Layer):\n",
    "    def __init__(self, num_sense, **kwargs):\n",
    "        self.init = initializations.get('glorot_uniform')\n",
    "        self.num_sense = num_sense\n",
    "        \n",
    "        super(MultiSense, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = R^{sentence_length x word_embedding}\n",
    "        # for each word, we have (1,num_sense) weight\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        # TODO: to determine the input data size, we need to use batches\n",
    "        # TODO: currently is the R^{num_sense x input_dim}, but desired R^{batch x num_sense x input_dim}\n",
    "        self.W = self.init((self.num_sense, input_dim),\n",
    "                             name='%s_W%d' % (self.name, i))\n",
    "        self.trainable_weights = self.Ws\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # x.shape = input_shape\n",
    "        # for each word, we elementwisely product with its weight\n",
    "        # for each word, we keep the row that has the highest softmax score\n",
    "        output = tf.mul(x, self.W)\n",
    "        return output\n",
    "        \n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "\n",
    "def create_shared_layer(output_dim, batch_input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(MultiSense(60, input_shape=batch_input_dim))\n",
    "    model.add(LSTM(output_dim))\n",
    "    return model\n",
    "\n",
    "shared_layer = create_shared_layer(128, (16, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Graph\n",
    "from keras.layers import Lambda, Activation\n",
    "\n",
    "batch_size = 16\n",
    "original_dim = 784  # sentence length\n",
    "intermediate_dim = 128  # encode size\n",
    "\n",
    "\n",
    "g = Graph()\n",
    "g.add_input(name='left', batch_input_shape=(batch_size, original_dim))\n",
    "g.add_input(name='right', batch_input_shape=(batch_size, original_dim))\n",
    "\n",
    "shared_layer = create_shared_layer(intermediate_dim, (batch_size, original_dim))\n",
    "g.add_shared_node(shared_layer,\n",
    "                  name='shared',\n",
    "                  inputs=['left', 'right'],\n",
    "                  merge_mode='cos',\n",
    "                  create_output=False)\n",
    "g.add_node(Activation('sigmoid'),\n",
    "           name='d',\n",
    "           input='shared')\n",
    "g.add_output(name='output', \n",
    "             input='d')\n",
    "g.compile(optimizer='RMSprop', loss={'output': 'mse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shared_layer.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is the draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet as F\n",
    "import theano\n",
    "\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, ins, n_in, n_out, w=None, b=None, f=T.tanh):\n",
    "        # initial weight and bias\n",
    "        if w is not isinstance(w, np.ndarray):\n",
    "            w = np.asarray(\n",
    "                    rng.uniform(\n",
    "                        low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                        high=np.sqrt(6. / (n_in + n_out)),\n",
    "                        size=(n_in, n_out)\n",
    "                    )\n",
    "                )\n",
    "            if f == theano.tensor.nnet.sigmoid:\n",
    "                w *= 4\n",
    "        if b is not isinstance(b, np.ndarray):\n",
    "            b = np.zeros((n_out,))\n",
    "       \n",
    "        w = theano.shared(w, borrow=True)\n",
    "        b = theano.shared(b, borrow=True)\n",
    "        \n",
    "        self.outs = T.dot(ins, w) - b\n",
    "        if f is not None:\n",
    "            # not a linear layer\n",
    "            self.outs = f(self.outs)\n",
    "        self.params = [w, b]\n",
    "        \n",
    "\n",
    "class NNet(object):\n",
    "    def __init__(self, data, gstep=0.01, epochs=1000, rng=None):\n",
    "        self.data = data\n",
    "        self.ins = data\n",
    "        self.n_sample, self.n_in = data.shape\n",
    "        self.gstep = gstep   # gradient step\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.epochs = epochs\n",
    "        if rng is None:\n",
    "            self.rng = np.random\n",
    "        else:\n",
    "            self.rng = rng\n",
    "        \n",
    "    def add_layer(self, n_out, w=None, b=None, f=T.tanh):\n",
    "        layer = HiddenLayer(rng=self.rng,\n",
    "                            ins=self.ins,\n",
    "                            n_in=self.n_in,\n",
    "                            n_out=n_out,\n",
    "                            w=w, b=b, f=f)\n",
    "        self.layers.append(layer)\n",
    "        # the output of this layer is the input of next layer\n",
    "        self.ins = layer.outs\n",
    "        self.n_in = n_out\n",
    "        self.params += layer.params\n",
    "        \n",
    "    def train(self):\n",
    "        x = T.dmatrix(\"x\")\n",
    "        # calculate the gradients\n",
    "        gparams = T.grad(cost, [param for param in self.params])\n",
    "        updates = [(param, param - self.gstep * gparam) for param, gparam in zip(self.params, gparams)]\n",
    "        _train = theano.function(\n",
    "                  inputs=[x],\n",
    "                  outputs=self.cost,\n",
    "                  updates=updates,\n",
    "                  on_unused_input='ignore')\n",
    "        for i in range(self.epochs):\n",
    "            err = _train(self.data)\n",
    "            print(\"Epoch: %d; Distance: %f\" %(i+1, err))\n",
    "        \n",
    "    def set_cost(self, cost):\n",
    "        self.cost = cost\n",
    "    \n",
    "    def get_final_outs(self):\n",
    "        return self.layers[-1].outs\n",
    "    \n",
    "\n",
    "N = 5 # training sample size\n",
    "IN_FEATS = 50 # input feature space\n",
    "EPOCHS = 1000 # train iteration\n",
    "\n",
    "rng = np.random\n",
    "D = (rng.randn(N, IN_FEATS))      # inputs\n",
    "\n",
    "nnet = NNet(D)\n",
    "# encoder\n",
    "nnet.add_layer(100)\n",
    "# decoder\n",
    "nnet.add_layer(IN_FEATS)\n",
    "cost = ((nnet.get_final_outs() - D) ** 2).sum()\n",
    "nnet.set_cost(cost)\n",
    "nnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet as F\n",
    "import theano\n",
    "\n",
    "N = 5 # training sample size\n",
    "IN_FEATS = 50 # input feature space, for encoder\n",
    "OUT_FEATS = 100 # output feature space, for decoder\n",
    "EPOCHS = 1000\n",
    "\n",
    "rng = np.random\n",
    "D = (rng.randn(N, IN_FEATS))      # inputs\n",
    "\n",
    "x = T.dmatrix(\"x\")\n",
    "\n",
    "e_w1 = theano.shared(rng.randn(IN_FEATS, OUT_FEATS), name=\"encode_w1\")  # weights\n",
    "e_b1 = theano.shared(np.zeros(OUT_FEATS), name=\"encode_b1\")             # bias\n",
    "d_w1 = theano.shared(rng.randn(OUT_FEATS, IN_FEATS), name=\"decode_w1\")\n",
    "d_b1 = theano.shared(np.zeros(IN_FEATS), name=\"decode_b1\")\n",
    "\n",
    "encoder = F.sigmoid(T.dot(x, e_w1) - e_b1)\n",
    "decoder = F.sigmoid(T.dot(encoder, d_w1) - d_b1)\n",
    "cost = ((decoder - x) ** 2).sum()\n",
    "e_gw1, e_gb1, d_gw1, d_gb1 = T.grad(cost, [e_w1, e_b1, d_w1, d_b1])\n",
    "\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          outputs=cost,\n",
    "          updates=[(e_w1, e_w1 - 0.01*e_gw1), (e_b1, e_b1 - 0.01*e_gb1),\n",
    "                   (d_w1, d_w1 - 0.01*d_gw1), (d_b1, d_b1 - 0.01*d_gb1)])\n",
    "predict = theano.function(inputs=[x], outputs=encoder)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    err = train(D)\n",
    "    print(\"Epoch: %d; Distance: %f\" %(i, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Psedocode of the CCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "func train_CCA(question_set, answer_set):\n",
    "    matrix = build_cross_covariance_matrix(question_set, answer_set)\n",
    "    U, s, V = svd(matrix)\n",
    "    return U, V\n",
    "```\n",
    "\n",
    "```\n",
    "func build_cross_covariance_matrix(question_set, answer_set):\n",
    "    pair_num, M = question_set.shape\n",
    "    pair_num, N = answer_set.shape\n",
    "    indx = 0\n",
    "    forEach question_set:\n",
    "        avg = average(question_set[indx])\n",
    "        question_set[indx] -= avg\n",
    "        avg = average(answer_set[indx])\n",
    "        answer_set[indx] -= avg\n",
    "        i++\n",
    "        \n",
    "    initial matrix \\in R^M x N fills by 0\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            matrix[i, j] = sum(question_set[i] \\dot answer_set[j].T) / (pair_num - 1)\n",
    "    return matrix\n",
    "```\n",
    "\n",
    "```\n",
    "func find_best_answer(question, answer_set, U, V):\n",
    "    question_project = question \\dot U\n",
    "    initial best_answer as NULL\n",
    "    initial best_similarity as INFINITY\n",
    "    indx = 0\n",
    "    forEach answer in answer_set:\n",
    "        answer_project = answer \\dot V\n",
    "        similarity = cosine_distance(question_project, answer_project)\n",
    "        if similarity <= best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_answer = indx\n",
    "        indx++\n",
    "    return indx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:24: RuntimeWarning: invalid value encountered in power\n/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:26: RuntimeWarning: invalid value encountered in power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cosine\n",
    "import logging\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "Qs = np.random.randn(80, 120)\n",
    "As = np.random.randn(80, 150)\n",
    "\n",
    "\n",
    "def train(Qs, As, is_sparse=True):\n",
    "    '''\n",
    "    params q: sentence embedding for question set\n",
    "    params a: sentence embedding for answer set\n",
    "    '''\n",
    "    if isinstance(Qs, list):\n",
    "        Qs = np.asarray(Qs, dtype=\"float32\")\n",
    "    if isinstance(As, list):\n",
    "        As = np.asarray(As, dtype=\"float32\")\n",
    "\n",
    "    logging.info(\"computing CCA\")\n",
    "    sample_num = Qs.shape[0]\n",
    "    c_qq_sqrt = np.power(Qs.T.dot(Qs), -0.5) / sample_num\n",
    "    c_qa = Qs.T.dot(As) / sample_num\n",
    "    c_aa_sqrt = np.power(As.T.dot(As), -0.5) / sample_num\n",
    "    # keep only diagonal\n",
    "    c_qq_sqrt = np.diag(np.diag(c_qq_sqrt))\n",
    "    c_aa_sqrt = np.diag(np.diag(c_aa_sqrt))\n",
    "    # get result\n",
    "    result = c_qq_sqrt.dot(c_qa).dot(c_aa_sqrt)\n",
    "    # U, s, V = np.linalg.svd(result, full_matrices=False)\n",
    "    U, s, V = svds(result, k=110)\n",
    "    Q_k = c_qq_sqrt.dot(U)\n",
    "    A_k = c_aa_sqrt.dot(V.T)\n",
    "    return Q_k, A_k\n",
    "\n",
    "\n",
    "# get distance between the question and answer, return with the answer index\n",
    "# def distance(indx_a, proj_q, A_k):\n",
    "#     indx, a = indx_a\n",
    "#     proj_a = A_k.T.dot(a)\n",
    "#     dist = cosine(proj_q, proj_a)\n",
    "#     return indx, dist\n",
    "# \n",
    "# \n",
    "# def find_answer(q, As, Q_k, A_k):\n",
    "#     proj_q = Q_k.T.dot(q)\n",
    "#     assert (proj_q == q.dot(Q_k)).all()\n",
    "#     with Pool(processes=8) as pool:\n",
    "#         result = pool.map(partial(distance, proj_q=proj_q, A_k=A_k), enumerate(As))\n",
    "#     best_indx, _ = min(result, key=lambda x: x[1])\n",
    "# \n",
    "#     return best_indx\n",
    "\n",
    "\n",
    "# get distance between the question and answer, return with the answer index\n",
    "def distance(indx_a, proj_q):\n",
    "    indx, proj_a = indx_a\n",
    "    dist = cosine(proj_q, proj_a)\n",
    "    return indx, dist\n",
    "\n",
    "\n",
    "def find_answer(proj_q, proj_As):\n",
    "    with Pool(processes=8) as pool:\n",
    "        result = pool.map(partial(distance, proj_q=proj_q), enumerate(proj_As))\n",
    "    best_indx, _ = min(result, key=lambda x: x[1])\n",
    "\n",
    "    return best_indx\n",
    "\n",
    "Q_k, A_k = train(Qs, As)\n",
    "\n",
    "# for i in range(Qs.shape[0]):\n",
    "#     print(find_answer(Qs[i], As, Q_k, A_k))\n",
    "    \n",
    "proj_Qs = np.tensordot(Qs, Q_k, axes=1)\n",
    "proj_As = np.tensordot(As, A_k, axes=1)\n",
    "for i in range(Qs.shape[0]):\n",
    "    print(find_answer(proj_Qs[i], proj_As))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
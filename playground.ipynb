{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is building the sentence autoencoder using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "variational autoencoder\n",
    "Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "\n",
    "batch_size = 16\n",
    "original_dim = 784  # sentence length\n",
    "latent_dim = 2\n",
    "intermediate_dim = 128  # encode size\n",
    "epsilon_std = 0.01\n",
    "nb_epoch = 40\n",
    "\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_std = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_std = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_std) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_std])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_std])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_std - K.square(z_mean) - K.exp(z_log_std), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_mean) \n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae.fit(_, _,\n",
    "        shuffle=True,\n",
    "        nb_epoch=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(_, _),\n",
    "        callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is building the sentence siamese network using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-14402db53b2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mshared_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_shared_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-14402db53b2e>\u001b[0m in \u001b[0;36mcreate_shared_layer\u001b[0;34m(output_dim, batch_input_dim)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_shared_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_input_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiSense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_input_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0minput_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_input_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mcreate_input_layer\u001b[0;34m(self, batch_input_shape, input_dtype, name)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    456\u001b[0m                                     '`layer.build(batch_input_shape)`')\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-14402db53b2e>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# input_shape = R^{sentence_length x word_embedding}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# for each word, we have (1,num_sense) weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         self.Ws = self.init((self.num_sense, input_dim),\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras.layers import initializations\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# multi-sense vectors\n",
    "class MultiSense(Layer):\n",
    "    def __init__(self, num_sense, **kwargs):\n",
    "        self.init = initializations.get('glorot_uniform')\n",
    "        self.num_sense = num_sense\n",
    "        \n",
    "        super(MultiSense, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = R^{sentence_length x word_embedding}\n",
    "        # for each word, we have (1,num_sense) weight\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        # TODO: to determine the input data size, we need to use batches\n",
    "        # TODO: currently is the R^{num_sense x input_dim}, but desired R^{batch x num_sense x input_dim}\n",
    "        self.W = self.init((self.num_sense, input_dim),\n",
    "                             name='%s_W%d' % (self.name, i))\n",
    "        self.trainable_weights = self.Ws\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # x.shape = input_shape\n",
    "        # for each word, we elementwisely product with its weight\n",
    "        # for each word, we keep the row that has the highest softmax score\n",
    "        output = tf.mul(x, self.W)\n",
    "        return output\n",
    "        \n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "\n",
    "def create_shared_layer(output_dim, batch_input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(MultiSense(60, input_shape=batch_input_dim))\n",
    "    model.add(LSTM(output_dim))\n",
    "    return model\n",
    "\n",
    "shared_layer = create_shared_layer(128, (16, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Graph\n",
    "from keras.layers import Lambda, Activation\n",
    "\n",
    "batch_size = 16\n",
    "original_dim = 784  # sentence length\n",
    "intermediate_dim = 128  # encode size\n",
    "\n",
    "\n",
    "g = Graph()\n",
    "g.add_input(name='left', batch_input_shape=(batch_size, original_dim))\n",
    "g.add_input(name='right', batch_input_shape=(batch_size, original_dim))\n",
    "\n",
    "shared_layer = create_shared_layer(intermediate_dim, (batch_size, original_dim))\n",
    "g.add_shared_node(shared_layer,\n",
    "                  name='shared',\n",
    "                  inputs=['left', 'right'],\n",
    "                  merge_mode='cos',\n",
    "                  create_output=False)\n",
    "g.add_node(Activation('sigmoid'),\n",
    "           name='d',\n",
    "           input='shared')\n",
    "g.add_output(name='output', \n",
    "             input='d')\n",
    "g.compile(optimizer='RMSprop', loss={'output': 'mse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shared_layer.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is the draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet as F\n",
    "import theano\n",
    "\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, ins, n_in, n_out, w=None, b=None, f=T.tanh):\n",
    "        # initial weight and bias\n",
    "        if w is not isinstance(w, np.ndarray):\n",
    "            w = np.asarray(\n",
    "                    rng.uniform(\n",
    "                        low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                        high=np.sqrt(6. / (n_in + n_out)),\n",
    "                        size=(n_in, n_out)\n",
    "                    )\n",
    "                )\n",
    "            if f == theano.tensor.nnet.sigmoid:\n",
    "                w *= 4\n",
    "        if b is not isinstance(b, np.ndarray):\n",
    "            b = np.zeros((n_out,))\n",
    "       \n",
    "        w = theano.shared(w, borrow=True)\n",
    "        b = theano.shared(b, borrow=True)\n",
    "        \n",
    "        self.outs = T.dot(ins, w) - b\n",
    "        if f is not None:\n",
    "            # not a linear layer\n",
    "            self.outs = f(self.outs)\n",
    "        self.params = [w, b]\n",
    "        \n",
    "\n",
    "class NNet(object):\n",
    "    def __init__(self, data, gstep=0.01, epochs=1000, rng=None):\n",
    "        self.data = data\n",
    "        self.ins = data\n",
    "        self.n_sample, self.n_in = data.shape\n",
    "        self.gstep = gstep   # gradient step\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.epochs = epochs\n",
    "        if rng is None:\n",
    "            self.rng = np.random\n",
    "        else:\n",
    "            self.rng = rng\n",
    "        \n",
    "    def add_layer(self, n_out, w=None, b=None, f=T.tanh):\n",
    "        layer = HiddenLayer(rng=self.rng,\n",
    "                            ins=self.ins,\n",
    "                            n_in=self.n_in,\n",
    "                            n_out=n_out,\n",
    "                            w=w, b=b, f=f)\n",
    "        self.layers.append(layer)\n",
    "        # the output of this layer is the input of next layer\n",
    "        self.ins = layer.outs\n",
    "        self.n_in = n_out\n",
    "        self.params += layer.params\n",
    "        \n",
    "    def train(self):\n",
    "        x = T.dmatrix(\"x\")\n",
    "        # calculate the gradients\n",
    "        gparams = T.grad(cost, [param for param in self.params])\n",
    "        updates = [(param, param - self.gstep * gparam) for param, gparam in zip(self.params, gparams)]\n",
    "        _train = theano.function(\n",
    "                  inputs=[x],\n",
    "                  outputs=self.cost,\n",
    "                  updates=updates,\n",
    "                  on_unused_input='ignore')\n",
    "        for i in range(self.epochs):\n",
    "            err = _train(self.data)\n",
    "            print(\"Epoch: %d; Distance: %f\" %(i+1, err))\n",
    "        \n",
    "    def set_cost(self, cost):\n",
    "        self.cost = cost\n",
    "    \n",
    "    def get_final_outs(self):\n",
    "        return self.layers[-1].outs\n",
    "    \n",
    "\n",
    "N = 5 # training sample size\n",
    "IN_FEATS = 50 # input feature space\n",
    "EPOCHS = 1000 # train iteration\n",
    "\n",
    "rng = np.random\n",
    "D = (rng.randn(N, IN_FEATS))      # inputs\n",
    "\n",
    "nnet = NNet(D)\n",
    "# encoder\n",
    "nnet.add_layer(100)\n",
    "# decoder\n",
    "nnet.add_layer(IN_FEATS)\n",
    "cost = ((nnet.get_final_outs() - D) ** 2).sum()\n",
    "nnet.set_cost(cost)\n",
    "nnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet as F\n",
    "import theano\n",
    "\n",
    "N = 5 # training sample size\n",
    "IN_FEATS = 50 # input feature space, for encoder\n",
    "OUT_FEATS = 100 # output feature space, for decoder\n",
    "EPOCHS = 1000\n",
    "\n",
    "rng = np.random\n",
    "D = (rng.randn(N, IN_FEATS))      # inputs\n",
    "\n",
    "x = T.dmatrix(\"x\")\n",
    "\n",
    "e_w1 = theano.shared(rng.randn(IN_FEATS, OUT_FEATS), name=\"encode_w1\")  # weights\n",
    "e_b1 = theano.shared(np.zeros(OUT_FEATS), name=\"encode_b1\")             # bias\n",
    "d_w1 = theano.shared(rng.randn(OUT_FEATS, IN_FEATS), name=\"decode_w1\")\n",
    "d_b1 = theano.shared(np.zeros(IN_FEATS), name=\"decode_b1\")\n",
    "\n",
    "encoder = F.sigmoid(T.dot(x, e_w1) - e_b1)\n",
    "decoder = F.sigmoid(T.dot(encoder, d_w1) - d_b1)\n",
    "cost = ((decoder - x) ** 2).sum()\n",
    "e_gw1, e_gb1, d_gw1, d_gb1 = T.grad(cost, [e_w1, e_b1, d_w1, d_b1])\n",
    "\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          outputs=cost,\n",
    "          updates=[(e_w1, e_w1 - 0.01*e_gw1), (e_b1, e_b1 - 0.01*e_gb1),\n",
    "                   (d_w1, d_w1 - 0.01*d_gw1), (d_b1, d_b1 - 0.01*d_gb1)])\n",
    "predict = theano.function(inputs=[x], outputs=encoder)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    err = train(D)\n",
    "    print(\"Epoch: %d; Distance: %f\" %(i, err))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
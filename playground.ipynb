{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is building the sentence autoencoder using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "variational autoencoder\n",
    "Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "\n",
    "batch_size = 16\n",
    "original_dim = 784  # sentence length\n",
    "latent_dim = 2\n",
    "intermediate_dim = 128  # encode size\n",
    "epsilon_std = 0.01\n",
    "nb_epoch = 40\n",
    "\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_std = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_std = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_std) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_std])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_std])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_std - K.square(z_mean) - K.exp(z_log_std), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_mean) \n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae.fit(_, _,\n",
    "        shuffle=True,\n",
    "        nb_epoch=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(_, _),\n",
    "        callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n",
    "\n",
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is building the sentence siamese network using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras.layers import initializations\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# multi-sense vectors\n",
    "class MultiSense(Layer):\n",
    "    def __init__(self, num_sense, **kwargs):\n",
    "        self.init = initializations.get('glorot_uniform')\n",
    "        self.num_sense = num_sense\n",
    "        \n",
    "        super(MultiSense, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = R^{sentence_length x word_embedding}\n",
    "        # for each word, we have (1,num_sense) weight\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        # TODO: to determine the input data size, we need to use batches\n",
    "        # TODO: currently is the R^{num_sense x input_dim}, but desired R^{batch x num_sense x input_dim}\n",
    "        self.W = self.init((self.num_sense, input_dim),\n",
    "                             name='%s_W%d' % (self.name, i))\n",
    "        self.trainable_weights = self.Ws\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # x.shape = input_shape\n",
    "        # for each word, we elementwisely product with its weight\n",
    "        # for each word, we keep the row that has the highest softmax score\n",
    "        output = tf.mul(x, self.W)\n",
    "        return output\n",
    "        \n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "\n",
    "def create_shared_layer(output_dim, batch_input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(MultiSense(60, input_shape=batch_input_dim))\n",
    "    model.add(LSTM(output_dim))\n",
    "    return model\n",
    "\n",
    "shared_layer = create_shared_layer(128, (16, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Graph\n",
    "from keras.layers import Lambda, Activation\n",
    "\n",
    "batch_size = 16\n",
    "original_dim = 784  # sentence length\n",
    "intermediate_dim = 128  # encode size\n",
    "\n",
    "\n",
    "g = Graph()\n",
    "g.add_input(name='left', batch_input_shape=(batch_size, original_dim))\n",
    "g.add_input(name='right', batch_input_shape=(batch_size, original_dim))\n",
    "\n",
    "shared_layer = create_shared_layer(intermediate_dim, (batch_size, original_dim))\n",
    "g.add_shared_node(shared_layer,\n",
    "                  name='shared',\n",
    "                  inputs=['left', 'right'],\n",
    "                  merge_mode='cos',\n",
    "                  create_output=False)\n",
    "g.add_node(Activation('sigmoid'),\n",
    "           name='d',\n",
    "           input='shared')\n",
    "g.add_output(name='output', \n",
    "             input='d')\n",
    "g.compile(optimizer='RMSprop', loss={'output': 'mse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shared_layer.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is the draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet as F\n",
    "import theano\n",
    "\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, ins, n_in, n_out, w=None, b=None, f=T.tanh):\n",
    "        # initial weight and bias\n",
    "        if w is not isinstance(w, np.ndarray):\n",
    "            w = np.asarray(\n",
    "                    rng.uniform(\n",
    "                        low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                        high=np.sqrt(6. / (n_in + n_out)),\n",
    "                        size=(n_in, n_out)\n",
    "                    )\n",
    "                )\n",
    "            if f == theano.tensor.nnet.sigmoid:\n",
    "                w *= 4\n",
    "        if b is not isinstance(b, np.ndarray):\n",
    "            b = np.zeros((n_out,))\n",
    "       \n",
    "        w = theano.shared(w, borrow=True)\n",
    "        b = theano.shared(b, borrow=True)\n",
    "        \n",
    "        self.outs = T.dot(ins, w) - b\n",
    "        if f is not None:\n",
    "            # not a linear layer\n",
    "            self.outs = f(self.outs)\n",
    "        self.params = [w, b]\n",
    "        \n",
    "\n",
    "class NNet(object):\n",
    "    def __init__(self, data, gstep=0.01, epochs=1000, rng=None):\n",
    "        self.data = data\n",
    "        self.ins = data\n",
    "        self.n_sample, self.n_in = data.shape\n",
    "        self.gstep = gstep   # gradient step\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.epochs = epochs\n",
    "        if rng is None:\n",
    "            self.rng = np.random\n",
    "        else:\n",
    "            self.rng = rng\n",
    "        \n",
    "    def add_layer(self, n_out, w=None, b=None, f=T.tanh):\n",
    "        layer = HiddenLayer(rng=self.rng,\n",
    "                            ins=self.ins,\n",
    "                            n_in=self.n_in,\n",
    "                            n_out=n_out,\n",
    "                            w=w, b=b, f=f)\n",
    "        self.layers.append(layer)\n",
    "        # the output of this layer is the input of next layer\n",
    "        self.ins = layer.outs\n",
    "        self.n_in = n_out\n",
    "        self.params += layer.params\n",
    "        \n",
    "    def train(self):\n",
    "        x = T.dmatrix(\"x\")\n",
    "        # calculate the gradients\n",
    "        gparams = T.grad(cost, [param for param in self.params])\n",
    "        updates = [(param, param - self.gstep * gparam) for param, gparam in zip(self.params, gparams)]\n",
    "        _train = theano.function(\n",
    "                  inputs=[x],\n",
    "                  outputs=self.cost,\n",
    "                  updates=updates,\n",
    "                  on_unused_input='ignore')\n",
    "        for i in range(self.epochs):\n",
    "            err = _train(self.data)\n",
    "            print(\"Epoch: %d; Distance: %f\" %(i+1, err))\n",
    "        \n",
    "    def set_cost(self, cost):\n",
    "        self.cost = cost\n",
    "    \n",
    "    def get_final_outs(self):\n",
    "        return self.layers[-1].outs\n",
    "    \n",
    "\n",
    "N = 5 # training sample size\n",
    "IN_FEATS = 50 # input feature space\n",
    "EPOCHS = 1000 # train iteration\n",
    "\n",
    "rng = np.random\n",
    "D = (rng.randn(N, IN_FEATS))      # inputs\n",
    "\n",
    "nnet = NNet(D)\n",
    "# encoder\n",
    "nnet.add_layer(100)\n",
    "# decoder\n",
    "nnet.add_layer(IN_FEATS)\n",
    "cost = ((nnet.get_final_outs() - D) ** 2).sum()\n",
    "nnet.set_cost(cost)\n",
    "nnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet as F\n",
    "import theano\n",
    "\n",
    "N = 5 # training sample size\n",
    "IN_FEATS = 50 # input feature space, for encoder\n",
    "OUT_FEATS = 100 # output feature space, for decoder\n",
    "EPOCHS = 1000\n",
    "\n",
    "rng = np.random\n",
    "D = (rng.randn(N, IN_FEATS))      # inputs\n",
    "\n",
    "x = T.dmatrix(\"x\")\n",
    "\n",
    "e_w1 = theano.shared(rng.randn(IN_FEATS, OUT_FEATS), name=\"encode_w1\")  # weights\n",
    "e_b1 = theano.shared(np.zeros(OUT_FEATS), name=\"encode_b1\")             # bias\n",
    "d_w1 = theano.shared(rng.randn(OUT_FEATS, IN_FEATS), name=\"decode_w1\")\n",
    "d_b1 = theano.shared(np.zeros(IN_FEATS), name=\"decode_b1\")\n",
    "\n",
    "encoder = F.sigmoid(T.dot(x, e_w1) - e_b1)\n",
    "decoder = F.sigmoid(T.dot(encoder, d_w1) - d_b1)\n",
    "cost = ((decoder - x) ** 2).sum()\n",
    "e_gw1, e_gb1, d_gw1, d_gb1 = T.grad(cost, [e_w1, e_b1, d_w1, d_b1])\n",
    "\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          outputs=cost,\n",
    "          updates=[(e_w1, e_w1 - 0.01*e_gw1), (e_b1, e_b1 - 0.01*e_gb1),\n",
    "                   (d_w1, d_w1 - 0.01*d_gw1), (d_b1, d_b1 - 0.01*d_gb1)])\n",
    "predict = theano.function(inputs=[x], outputs=encoder)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    err = train(D)\n",
    "    print(\"Epoch: %d; Distance: %f\" %(i, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Psedocode of the CCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "func train_CCA(question_set, answer_set):\n",
    "    matrix = build_cross_covariance_matrix(question_set, answer_set)\n",
    "    U, s, V = svd(matrix)\n",
    "    return U, V\n",
    "```\n",
    "\n",
    "```\n",
    "func build_cross_covariance_matrix(question_set, answer_set):\n",
    "    pair_num, M = question_set.shape\n",
    "    pair_num, N = answer_set.shape\n",
    "    indx = 0\n",
    "    forEach question_set:\n",
    "        avg = average(question_set[indx])\n",
    "        question_set[indx] -= avg\n",
    "        avg = average(answer_set[indx])\n",
    "        answer_set[indx] -= avg\n",
    "        i++\n",
    "        \n",
    "    initial matrix \\in R^M x N fills by 0\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            matrix[i, j] = sum(question_set[i] \\dot answer_set[j].T) / (pair_num - 1)\n",
    "    return matrix\n",
    "```\n",
    "\n",
    "```\n",
    "func find_best_answer(question, answer_set, U, V):\n",
    "    question_project = question \\dot U\n",
    "    initial best_answer as NULL\n",
    "    initial best_similarity as INFINITY\n",
    "    indx = 0\n",
    "    forEach answer in answer_set:\n",
    "        answer_project = answer \\dot V\n",
    "        similarity = cosine_distance(question_project, answer_project)\n",
    "        if similarity <= best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_answer = indx\n",
    "        indx++\n",
    "    return indx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "1\n",
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "q = np.random.randn(5, 4)\n",
    "a = np.random.randn(5, 8)\n",
    "    \n",
    "def xcov(set1, set2):\n",
    "    num_set1, M = set1.shape\n",
    "    num_set2, N = set2.shape\n",
    "    # one-to-one paired\n",
    "    assert(num_set1 == num_set2)\n",
    "    \n",
    "    a = set1 - np.transpose([np.average(set1, axis=1)])\n",
    "    b = set2 - np.transpose([np.average(set2, axis=1)])\n",
    "    cov = np.dot(a.T, b) / (num_set1 - 1)\n",
    "    \n",
    "#     # cross-covariance matrix\n",
    "#     cov = np.zeros((M, N), dtype='float64')\n",
    "#     for i in range(num_set1):\n",
    "#         for j in range(num_set1):\n",
    "#             cov[i, j] = np.cov(m1[i],m2[j])[0][1]\n",
    "            \n",
    "    return cov\n",
    "\n",
    "class CCA(object):\n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.V = None\n",
    "    \n",
    "    def train(self, q, a):\n",
    "        '''\n",
    "        params q: sentence embedding for question set\n",
    "        params a: sentence embedding for answer set\n",
    "        '''\n",
    "        cov = xcov(q, a)\n",
    "        self.U, s, self.V = np.linalg.svd(cov, full_matrices=False)\n",
    "    \n",
    "    def find_answer(self, v_q, a):\n",
    "        assert self.U is not None and \\\n",
    "               self.V is not None\n",
    "        v_q_proj = np.dot(v_q, self.U)\n",
    "        \n",
    "        best_distance = np.inf\n",
    "        best_indx = 0\n",
    "        for i, v_a in enumerate(a):\n",
    "            v_a_proj = np.dot(v_a, self.V.T)\n",
    "            s = cosine(v_q_proj, v_a_proj)\n",
    "            if s <= best_distance:\n",
    "                best_distance = s\n",
    "                best_indx = i\n",
    "        return best_indx\n",
    "\n",
    "model = CCA()\n",
    "model.train(q, a)\n",
    "for v_q in q:\n",
    "    print(model.find_answer(v_q, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
